![image](https://github.com/YangyangQu/Sign_Language_Datasets/assets/44865046/417e2d25-5f6f-4b61-a7fd-4eec59005979)![image](https://github.com/YangyangQu/Sign_Language_Datasets/assets/44865046/77badc4e-5bce-4c54-80e3-cd6a9d9c100f)# Sign-Language-Dataset
## This repository collects some common sign language recognition datasets.

| Dataset | Download Link | Paper Link | Length | Type | Country |
|-----|-----|-----|-----|-----|-----|
| WLASL |[link](https://dxli94.github.io/WLASL/)| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |2,000 words | Video | American | 
| MSASL |[link]([https://dxli94.github.io/WLASL/](https://www.microsoft.com/en-us/research/project/ms-asl/))| |1000 class | Video |  | 
| NMFs-CSL |[link]([https://dxli94.github.io/WLASL/](https://ustc-slr.github.io/datasets/2020_nmfs_csl/))| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |1,067 words | Video | Chinese | 
| SLR500 |[link](https://dxli94.github.io/WLASL/)| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |内容1 | Video | American | 
| Slovo |[link](https://dxli94.github.io/WLASL/)| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |内容1 | Video | American | 
| BOBSL |[link](https://dxli94.github.io/WLASL/)| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |内容1 | Video | American | 
| WLASL |[link](https://dxli94.github.io/WLASL/)| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |内容1 | Video | American | 
| WLASL |[link](https://dxli94.github.io/WLASL/)| [link](https://arxiv.org/pdf/1910.11006v2.pdf) |内容1 | Video | American | 


| 列1 | 列2 |
|-----|-----|
| 内容1 | 内容2 |
*WLASL: 14,289, 3,916, and 2,878 video segments in the train, dev, and test splits, respectively. [Link](https://dxli94.github.io/WLASL/)

+MSASL: 16,054, 5,287, and 4,172 video segments in the train, dev, and test splits, respectively. [Link](https://www.microsoft.com/en-us/research/project/ms-asl/)
-NMFs-CSL: 25,608 and 6,402 video segments in the train and test splits, respectively. [Link]
SLR500: 90,000 and 35,000 video segments in the train and test splits, respectively. [Link]
Slovo: 15,300 and 5,100 video segments in the train and test splits, respectively. [Link]
BOBSL: 993,000, 20,000, 165,000 video segments in train, val and test splits, respectively. [Link]
Continue sign language recognition datasets:

Phoenix-2014: 5,672, 540 and 629 video segments in the train, dev, and test splits, respectively. [Link]
Phoenix-2014T: 7,096, 519 and 642 video segments in train, dev and test splits, respectively. [Link]
Sign language translation datasets:

Phoenix-2014T: 7,096, 519 and 642 video segments in train, dev and test splits, respectively. [Link]
CSL-Daily: 18,401, 1,077 and 1,176 video segments in train, dev and test splits, respectively. [Link]
OpenASL: 96,476, 966 and 975 video segments in train, val and test splits, respectively. [Link]
How2Sign: 31,128, 1,741, 2,322 video segments in train, val and test splits, respectively. [Link]
BOBSL: 993,000, 20,000, 165,000 video segments in train, val and test splits, respectively. [Link]
